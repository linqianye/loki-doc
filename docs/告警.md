# 告警

Loki 包含一个名为 Ruler 的组件，它改编自我们的上游项目 Cortex。Ruler 负责持续评估一组可配置的查询，然后在某些情况发生时发出警报，例如高百分比的错误日志。

首先，确保启用了Ruler组件。以下是从配置文件加载规则的基本配置：

```yaml
ruler:
  storage:
    type: local
    local:
      directory: /tmp/rules
  rule_path: /tmp/scratch
  alertmanager_url: http://localhost
  ring:
    kvstore:
      store: inmemory
  enable_api: true
```

# 与Prometheus兼容

运行Ruler（默认情况下是以单个二进制文件运行）时，Loki 接受规则文件，然后调度它们进行持续评估。这些是与Prometheus兼容的！这意味着规则文件的结构与Prometheus的警报规则相同，只是指定的规则在LogQL中。

让我们看看它是什么样子的：

规则文件的语法为：

```yaml
groups:
  [ - <rule_group> ]
```

一个简单的示例文件：

```yaml
groups:
  - name: example
    rules:
    - alert: HighThroughputLogStreams
      expr: sum by(container) (rate({job=~"loki-dev/.*"}[1m])) > 1000
      for: 2m
```

## `<rule_group>`

```yaml
# The name of the group. Must be unique within a file.
name: <string>

# How often rules in the group are evaluated.
[ interval: <duration> | default = Ruler.evaluation_interval || 1m ]

rules:
  [ - <rule> ... ]
```

## `<rule>`

警报规则的语法是（有关更多详细信息，请参阅 LogQL[指标查询](https://grafana.com/docs/loki/v2.2.1/logql/#metric-queries)）：

```yaml
# The name of the alert. Must be a valid label value.
alert: <string>

# The LogQL expression to evaluate (must be an instant vector). Every evaluation cycle this is
# evaluated at the current time, and all resultant time series become
# pending/firing alerts.
expr: <string>

# Alerts are considered firing once they have been returned for this long.
# Alerts which have not yet fired for long enough are considered pending.
[ for: <duration> | default = 0s ]

# Labels to add or overwrite for each alert.
labels:
  [ <labelname>: <tmpl_string> ]

# Annotations to add to each alert.
annotations:
  [ <labelname>: <tmpl_string> ]
```

## 示例

规则文件的完整示例如下所示：

```yaml
groups:
  - name: should_fire
    rules:
      - alert: HighPercentageError
        expr: |
          sum(rate({app="foo", env="production"} |= "error" [5m])) by (job)
            /
          sum(rate({app="foo", env="production"}[5m])) by (job)
            > 0.05
        for: 10m
        labels:
            severity: page
        annotations:
            summary: High request latency
  - name: credentials_leak
    rules: 
      - alert: http-credentials-leaked
        annotations: 
          message: "{{ $labels.job }} is leaking http basic auth credentials."
        expr: 'sum by (cluster, job, pod) (count_over_time({namespace="prod"} |~ "http(s?)://(\\w+):(\\w+)@" [5m]) > 0)'
        for: 10m
        labels: 
          severity: critical
```

# 用例

Ruler 的 Prometheus 兼容性进一步强调了指标和日志之间的结合。对于那些希望基于日志发出警报或想知道为什么这样的人可能有用，这里有一些我们认为非常适合的用例。

## 我们还没有使用指标

许多新兴项目、应用程序甚至公司可能还没有指标后端。我们倾向于在指标支持之前添加日志支持，因此如果您处于此阶段，基于日志的警报可以帮助弥补这一差距。为错误日志的百分比（如前面的示例）构建Loki警报很容易，例如之前的示例：

```yaml
- alert: HighPercentageError
  expr: |
  sum(rate({app="foo", env="production"} |= "error" [5m])) by (job)
    /
  sum(rate({app="foo", env="production"}[5m])) by (job)
    > 0.05
```

## 黑盒监控

我们并不总是能控制我们运行的应用程序的源代码。想想负载均衡器和无数支持我们应用程序的组件（包括开源和封闭的第三方）；不公开您想要的指标（或根本没有任何指标）是一个常见问题。那么，我们如何才能将它们纳入我们的可观察性堆栈中以便有效地监控它们？基于日志的警报是解决这些问题的好方法。

## 事件提醒

有时你想知道是否发生了什么事。基于日志的警报是处理此问题的一种很好的方法，例如以下是查找泄漏的身份验证凭据的示例：：

```yaml
- name: credentials_leak
  rules: 
    - alert: http-credentials-leaked
      annotations: 
        message: "{{ $labels.job }} is leaking http basic auth credentials."
      expr: 'sum by (cluster, job, pod) (count_over_time({namespace="prod"} |~ "http(s?)://(\\w+):(\\w+)@" [5m]) > 0)'
      for: 10m
      labels: 
        severity: critical
```

## 对高基数源发出警报

另一个很好的用例是对高基数源发出警报。这些东西记录为指标是很难并且昂贵的，因为潜在的标签集是巨大的。一个很好的例子是在多租户系统（如Loki）中按租户发出警报。这是一种常见的平衡行为，既需要每个租户的指标，也需要随之产生基数暴增（向现有 Prometheus 指标添加一个租户标签将增加租户数量的基数）。

在 LogQL 中创建这些警报很有吸引力，因为这些指标可以在查询时提取，这意味着我们不会在指标存储中遭受基数暴增。

> **注意** 举个例子，我们可以使用 LogQL v2 来帮助 Loki自我监控，当特定租户的查询需要超过10秒才能完成时提醒我们！为此，我们将使用以下查询：`sum by (org_id) (rate({job="loki-prod/query-frontend"} |= "metrics.go" | logfmt | duration > 10s [1m]))`

# 与Ruler互动

由于规则文件与 Prometheus 规则文件相同，我们可以通过[`cortex-tool`](https://github.com/grafana/cortex-tools#rules). 与Loki 规则进行交互。CLI还处于早期开发阶段，但可以与Loki和cortex一起工作。在与 Loki 一起使用时确保将参数`--backend=loki`传递给命令。

> **注意：**目前并非所有的 cortextool 命令都支持 Loki。

> **注意：** cortextool 旨在针对多租户Loki运行，命令需要将`--id=`标志设置为 Loki 实例ID或设置环境变量`CORTEX_TENANT_ID`。如果 Loki 在单租户模式下运行，所需的ID是`fake`。

下面包括一个工作流示例：

```sh
# lint the rules.yaml file ensuring it's valid and reformatting it if necessary
cortextool rules lint --backend=loki ./output/rules.yaml

# diff rules against the currently managed ruleset in Loki
cortextool rules diff --rule-dirs=./output --backend=loki

# ensure the remote ruleset matches your local ruleset, creating/updating/deleting remote rules which differ from your local specification.
cortextool rules sync --rule-dirs=./output --backend=loki

# print the remote ruleset
cortextool rules print --backend=loki
```

还有一个[github 操作](https://github.com/grafana/cortex-rules-action)可用于`cortex-tool`，因此您可以将其添加到您的 CI/CD 中！

例如，您可以通过以下方式同步主版本上的规则：

```yaml
name: sync-cortex-rules-and-alerts
on:
  push:
    branches:
      - master
env:
  CORTEX_ADDRESS: '<fill me in>'
  CORTEX_TENANT_ID: '<fill me in>'
  CORTEX_API_KEY: ${{ secrets.API_KEY }}
  RULES_DIR: 'output/'
jobs:
  sync-loki-alerts:
    runs-on: ubuntu-18.04
    steps:
      - name: Lint Rules
        uses: grafana/cortex-rules-action@v0.4.0
        env:
          ACTION: 'lint'
        with:
          args: --backend=loki
      - name: Diff rules
        uses: grafana/cortex-rules-action@v0.4.0
        env:
          ACTION: 'diff'
        with:
          args: --backend=loki
      - name: Sync rules
        if: ${{ !contains(steps.diff-rules.outputs.detailed, 'no changes detected') }}
        uses: grafana/cortex-rules-action@v0.4.0
        env:
          ACTION: 'sync'
        with:
          args: --backend=loki
      - name: Print rules
        uses: grafana/cortex-rules-action@v0.4.0
        env:
          ACTION: 'print'
```

# 调度和最佳实践

缩放Ruler的一个选项是水平缩放。但是，在运行多个Ruler实例时，它们需要进行协调以确定哪个实例来评估哪个规则。与ingester类似，Ruler建立一个哈希环来划分评估规则的职责。

[配置文档中](配置.md)完整列出了可能的配置，但为了在多个Ruler之间切分规则，必须通过标志 ( `-experimental.Ruler.enable-api`) 或配置文件参数启用规则 API 。其次，Ruler 需要配置自己的环。在那里，Ruler将自动分片和处理规则的划分。与ingester不同，Rulers 不移交责任：每次将 Ruler 添加到环或从环中删除时，所有规则都会随机重新分片。

一个完整的启用分片的 Ruler 示例是：

```yaml
ruler:
    alertmanager_url: <alertmanager_endpoint>
    enable_alertmanager_v2: true
    enable_api: true
    enable_sharding: true
    ring:
        kvstore:
            consul:
                host: consul.loki-dev.svc.cluster.local:8500
            store: consul
    rule_path: /tmp/rules
    storage:
        gcs:
            bucket_name: <loki-rules-bucket>
```

# Ruler的存储

Ruler 支持六种存储：configdb、azure、gcs、s3、swift 和 local。大多数类型的存储都以一种明文的方式与分片的 Ruler 配置一起使用，即将所有 Ruler 配置为使用相同的后端。

 local的实现是从本地文件系统读取规则文件。这是一个只读后端，不支持通过[Ruler API](https://grafana.com/docs/loki/v2.2.1/api/#Ruler)创建和删除规则。尽管它读取本地文件系统，但如果操作员小心地将相同的规则加载到每个Ruler，则该方法仍然可以在分片Ruler配置中使用。例如，这可以通过将[Kubernetes 配置映射](https://kubernetes.io/docs/concepts/configuration/configmap/)挂载到每个 Ruler pod 来实现。

典型的 local配置可能类似于：

```
  -ruler.storage.type=local
  -ruler.storage.local.directory=/tmp/loki/rules
```

使用上述配置，Ruler将期望以下布局：

```
/tmp/loki/rules/<tenant id>/rules1.yaml
                           /rules2.yaml
```

Yaml 文件应与[Prometheus 兼容](告警.md#与Prometheus兼容)，但包含本文档开头指定的 LogQL 表达式。

# 未来的改进

有一些事情可以提高这项服务的健壮性。不按特定顺序：

- 记录规则。
- 后端指标存储用于生成警报和记录规则数据的适配器。第一个可能是 Cortex，因为 Loki 建立在它之上。

# 其他详细信息：指标后端与内存

目前，Loki Ruler 与支持的 Prometheus 存储是解耦合的。通常，评估规则的结果以及警报状态的历史记录存储为时间序列。Loki 无法存储和检索这些来使其能够独立于即 Prometheus 运行。作为一种解决方法，Loki 在内存中保留一个小存储区，其目的是在重新调度或重排规则时延迟加载过去的评估。未来，Loki 将支持可选的指标后端，允许存储这些指标以获得审计和性能优势。