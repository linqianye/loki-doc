# Loki的架构

本文档是对 [Loki 概述](概述.md) 中详细介绍的信息的扩展。

## 多租户

当 Loki 以多租户模式运行时，内存和持久存储中的所有数据都按照租户ID进行分区，租户ID从请求的HTTP头中的`X-Scope-OrgID`字段提取。

`X-Scope-OrgID`当 Loki 在多租户模式下运行时，所有数据 - 内存中和长期存储中 - 都按租户 ID 进行分区，从请求中的HTTP 标头中提取。当 Loki不是多租户模式时，将忽略标头信息，并将出现在索引和存储的块中的租户ID设置为“false”。

## 操作模式

![模式图](https://grafana.com/docs/loki/latest/architecture/modes_of_operation.png)

Loki 有一组在内部被称为模块组件（在下面的[Components 中](https://grafana.com/docs/loki/latest/architecture/#components)定义）。每个组件生成一个用于内部通信的 gRPC 服务器和一个用于外部API请求的HTTP/1服务器。所有组件都带有一个 HTTP/1 服务器，但大多数组件只公开就绪性、运行状况和指标端点。

Loki 运行哪个组件由命令行中的`-target`标志或Loki 配置文件中的`target: <string>`部分决定。当值`target`为`all`时，Loki将在单个进程中运行所有组件。这被称为“单进程”、“单二进制”或单片模式。当使用 Helm 安装 Loki 时，单进程模式是 Loki 的默认部署。

当`target`没有设置为`all`（它被设置为`querier`，`ingester`， `query-frontend`，或`distributor`）时，则Loki被称作“分部署部署”，或微服务模式下运行。

Loki 的每个组件，例如 ingesters 和 distributors，都使用 Loki 配置中定义的 gRPC 通过 gRPC 监听端口相互通信。当以单进程模式运行组件时，每个组件虽然运行在同一进程中，但将通过本地网络相互连接以进行组件间通信，这仍然是正确的。

单进程模式非常适合本地开发、小工作负载和测试目的。进程模式可以通过多个进程进行扩展，但有以下限制：

1. 当运行具有多个副本的单体模式时，当前无法使用本地索引和本地存储，因为每个副本必须能够访问相同的存储后端，并且本地存储对于并发访问是不安全的。
2. 单个组件无法独立扩展，因此读取组件不可能多于写入组件。

## 组件

### Distributor

Distributor服务负责响应处理客户端发送的日志。它本质上是日志数据写入路径中的“第一站”。一旦 distributor 接收到日志数据，它就会将它们分成批次并并将它们并行的发送到多个[ingesters](概述.md#Ingester)。

有关更多信息，请参阅[Distributor](架构.md#Distributor)页面。

### 哈希散列

Distributors 使用一致的哈希和可配置的复制因子来确定 ingesters 服务的哪些实例应该接收日志数据。

流是一组与租户和唯一标签集关联的日志。使用租户 ID 和标签集对流进行哈希，然后使用哈希值来查找要将流发送到的ingesters。

利用[consul](https://www.consul.io/)中存储的哈希环实现一致性哈希；所有 ingesters 都使用他们拥有的一组令牌将自己注册到哈希环中。每个令牌都是一个随机的无符号 32 位数字。ingesters将他们的状态连同一组令牌注册到哈希环中。JOINING和 ACTIVE 状态的ingesters可以接收写入请求，而 ACTIVE 和 LEAVING状态的ingesters可以接收读取请求。在进行哈希查找时，distributors仅对处于请求状态的ingesters使用令牌。

为了进行哈希查找，distributors会找到值大于流的哈希的最小值当作令牌。当复制因子大于1时，属于不同 ingesters 的下一个后续令牌（环中顺时针方向）也将包含在结果中。

这种散列设置的效果是，ingesters拥有的每个令牌都负责一系列哈希散列。如果存在三个值为0、25和50的令牌，则将向拥有令牌25的 ingester 提供哈希值3；拥有令牌25的 ingester 负责1-25的哈希范围。

### 仲裁一致性

由于所有 distributor 共享对同一个哈希环的访问权限，因此可以将写入请求发送到任何一个distributor。

为确保查询结果一致，Loki 在读写操作时使用 [Dynamo 风格](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf) 的仲裁一致性。这意味着distributor将等待至少二分之一个周期加上一个 ingester 的时间，然后才能将响应数据发送给用户。

### Ingester

Ingester 服务负责将日志数据写入到持久存储的后端（DynamoDB、S3、Cassandra等）。

Ingester拥有一个生命周期器，它管理哈希环中Ingester的生命周期。每个ingester都处于`PENDING`，`JOINING`， `ACTIVE`，`LEAVING`，或`UNHEALTHY`中的某一个状态：

1. `PENDING`是一个Ingester在等待另一个正在离开的Ingester的切换时的状态。
2. `JOINING`是Ingester将其令牌插入环并初始化自身时的状态。它可以接收持有这个令牌的写入请求。
3. `ACTIVE`是完成初始化的 Ingester 的状态。它可以接收持有这个令牌的写入和读取请求。
4. `LEAVING`是 Ingester 关闭时的状态。它可以接收对内存中仍然存在的数据的读取请求。
5. `UNHEALTHY`是 Ingester 向 Consul 心跳失败时的状态。`UNHEALTHY`由distributor在定期检查环时设置。

Ingester接收到的每个日志流都在内存中构建为包含多个“块”的组，并以可配置刷新到持久存储后端的时间间隔。

在以下情况下，块会被压缩并标记为只读：

1. 当前块的容量已满（可配置值）。
2. 太长时间没有更新当前块
3. 发生刷新。

每当一个块被压缩并标记为只读时，一个可写的块就会取代它。

如果一个 ingester 进程崩溃或突然退出，所有尚未刷新的数据都将丢失。Loki 通常被配置为复制每个日志为多个副本（通常为 3 个）来降低这种风险。

当持久存储提供程序发生刷新时，将根据其租户、标签和内容对块进行哈希处理。这意味着具有相同数据副本的多个ingesters不会将相同的数据写入持久存储两次，但是如果对其中一个副本的任何写入失败，则会在持久存储中创建多个不同的块对象。有关如何消除重复数据，请参阅[Querier](架构.md#querier)。

ingester会验证收到的每个日志行的时间戳是否保持严格的顺序。有关时间戳顺序规则的详细文档，请参阅[Loki 概述](概述.md#时间戳排序)。

### 切换

默认情况下，当 ingester 关闭并尝试离开哈希环时，哈希环将等待查看是否有新的 ingester 在刷新之前尝试进入如果有则尝试启动切换。切换会将离开的 ingester 拥有的所有令牌和内存中的块转移到新的 ingester。

在加入哈希环之前，ingester将在`PENDING`状态下等待切换。在可配置的超时之后，处于`PENDING`状态且未收到传输状态的ingester将正常加入环，插入一组新的令牌。

这个过程用来避免在关闭时刷新所有的chunk，这是一个缓慢的过程。

### 查询前端

查询前端是一个可选服务，提供查询器的API接入点，可以用来加速读取路径。当查询前端就位时，传入的查询请求应该被定向到查询前端而不是查询器。集群内仍需要查询器服务来执行实际查询。

查询前端在内部执行一些查询调整并将查询保存在内部队列中。在此设置中，查询器充当从队列中提取作业、执行作业并将作业返回到查询前端进行聚合的工作器。查询器需要配置查询前端地址（通过`-querier.frontend-address`参数）来让它们连接到查询前端。

查询前端是**无状态的**。但是，由于内部队列的工作方式，建议运行一些查询前端副本来获得公平调度的好处。大多数情况下，两个副本就足够了。

#### 排队

查询前端排队机制用于：

- 确保可能导致查询器内存不足 (OOM) 错误的大型查询在失败时进行重试。这允许管理员为查询提供不足的内存，或者乐观地并行运行更多的小查询，这有助于降低 TCO。
- 通过使用FIFO将多个大型请求分发到所有查询器，防止将多个大型请求传送到单个查询器上。
- 通过公平地安排租户之间的查询，防止单个租户出现拒绝服务 (DOSing) 其他租户。

#### 拆分

查询前端将较大的查询拆分为多个较小的查询，在下游查询器上并行的执行这些查询并将结果重新拼接在一起。这可以防止大型查询在单个查询器中导致内存不足问题，并有助于更快地执行它们。

#### 缓存

##### 指标查询

查询前端支持缓存指标查询的结果并在后续查询中重用它们。如果缓存的结果不完整，查询前端会计算所需的子查询并在下游查询器上并行执行。查询前端可以选择将查询与其 step 参数对齐，来提高查询结果的可缓存性。缓存结果与所有 loki 缓存后端（目前是 memcached、redis 和内存缓存）都兼容。

##### 日志查询 - 即将推出！

正在积极开发缓存日志（filter，regexp）查询。

### Querier

Queryer（查询器）服务使用 [LogQL](logql.md)查询语言处理查询，从ingester和持久存储中获取日志。

查询器在回退到对后端存储运行相同的查询之前查询所有 ingesters 的内存数据。由于复制的因素，查询器可能会收到重复的数据。为了解决这个问题，查询器会在内部消除具有相同纳秒时间戳、标签集和日志消息的数据。

## Chunk的格式

```
  -------------------------------------------------------------------
  |                               |                                 |
  |        MagicNumber(4b)        |           version(1b)           |
  |                               |                                 |
  -------------------------------------------------------------------
  |         block-1 bytes         |          checksum (4b)          |
  -------------------------------------------------------------------
  |         block-2 bytes         |          checksum (4b)          |
  -------------------------------------------------------------------
  |         block-n bytes         |          checksum (4b)          |
  -------------------------------------------------------------------
  |                        #blocks (uvarint)                        |
  -------------------------------------------------------------------
  | #entries(uvarint) | mint, maxt (varint) | offset, len (uvarint) |
  -------------------------------------------------------------------
  | #entries(uvarint) | mint, maxt (varint) | offset, len (uvarint) |
  -------------------------------------------------------------------
  | #entries(uvarint) | mint, maxt (varint) | offset, len (uvarint) |
  -------------------------------------------------------------------
  | #entries(uvarint) | mint, maxt (varint) | offset, len (uvarint) |
  -------------------------------------------------------------------
  |                      checksum(from #blocks)                     |
  -------------------------------------------------------------------
  |                    #blocks section byte offset                  |
  -------------------------------------------------------------------
```

`mint`并分别`maxt`描述最小和最大的 Unix 纳秒时间戳。

### block的格式

一个block由一系列条目组成，每个条目都是一个单独的日志行。

请注意，block的字节使用 Gzip 压缩存储。以下是它们未压缩时的形式：

```
  -------------------------------------------------------------------
  |    ts (varint)    |     len (uvarint)    |     log-1 bytes      |
  -------------------------------------------------------------------
  |    ts (varint)    |     len (uvarint)    |     log-2 bytes      |
  -------------------------------------------------------------------
  |    ts (varint)    |     len (uvarint)    |     log-3 bytes      |
  -------------------------------------------------------------------
  |    ts (varint)    |     len (uvarint)    |     log-n bytes      |
  -------------------------------------------------------------------
```

`ts` 是日志的 Unix 纳秒时间戳，而 `len `是日志条目的字节长度。

## Chunk存储

块存储是Loki的持久数据存储，旨在支持交互式查询和持续写入，无需后台维护任务。它包括：

- 块的索引。该索引可以由以下方式支持：
  - [Amazon DynamoDB](https://aws.amazon.com/dynamodb)
  - [Google Bigtable](https://cloud.google.com/bigtable)
  - [Apache Cassandra](https://cassandra.apache.org/)
- 块数据本身的键值 (KV) 存储，可以是：
  - [Amazon DynamoDB](https://aws.amazon.com/dynamodb)
  - [Google Bigtable](https://cloud.google.com/bigtable)
  - [Apache Cassandra](https://cassandra.apache.org/)
  - [Amazon S3](https://aws.amazon.com/s3)
  - [Google Cloud Storage](https://cloud.google.com/storage/)

> 与 Loki 的其他核心组件不同，块存储不是一个单独的服务、作业或进程，而是一个嵌入在需要访问 Loki 数据的两个服务（[ingester](https://grafana.com/docs/loki/v2.2.0/overview/#ingester)和[querier](https://grafana.com/docs/loki/v2.2.0/overview/#querier)）中的库。

块存储依赖于可用于支持块存储索引的“ [NoSQL](https://en.wikipedia.org/wiki/NoSQL) ”存储（DynamoDB、Bigtable 和 Cassandra）的统一接口。此接口假定索引是由以下项键入的条目的集合：

- **hash key**。这是所有读写操作所必需的。
- **range key**。这是写操作所必需的，读操作时可以省略，读操作可以通过前缀或范围查询。

该接口在支持的数据库中的工作方式略有不同：

- DynamoDB 本身支持范围和哈希。因此，索引条目直接建模为 DynamoDB 条目，散列键作为分布键，范围作为范围键。
- 对于 Bigtable 和 Cassandra，索引条目被建模为单独的列值。哈希键成为行键，范围键成为列键。

模式是用于将读写块存储时使用的匹配器和标签集映射到索引上的适当操作。随着 Loki 的发展，增加了模式，主要是为了更好地平衡写操作和提高查询性能。

## 读取的流程

总而言之，日志读取流程的工作方式如下：

1. 查询器收到 HTTP/1 数据请求。
2. 查询器将查询传递给内存中数据的所有ingesters。
3. ingesters接收读取请求并返回与查询匹配的数据（如果有）。
4. 如果没有ingesters返回数据，查询器会紧接着从持久存储中加载数据并对它运行查询。
5. 查询器对所有接收到的数据进行迭代并删除重复数据，通过HTTP/1连接返回一组最终数据。

## 写入的流程

![块图](https://grafana.com/docs/loki/latest/architecture/chunks_diagram.png)

总而言之，日志写入流程的工作方式如下：

1. Distributor收到HTTP/1请求后存储流的数据。
2. 每个流都使用哈希环进行哈希。
3. Distributor将每个流发送到适当的ingester及其副本（基于配置的复制因子）。
4. 每个ingester将为流的数据创建一个块或附加到现有块。每个租户和每个标签集的块都是唯一的。
5. Distributor通过 HTTP/1 连接用成功代码进行响应。



# Distributor组件

## 它在哪里？

Distributor 是 Loki 写入流程下游的第一个组件，用于所有网关提供身份验证和负载均衡。它负责在将传入数据发送到 ingester 组件之前对其进行验证、预处理和应用速率限制集。重要的是，为了正确负载均衡流量，负载均衡器位于distributor的前面。

## 它有什么作用？

### 验证

Distributor的第一步是确保所有传入的数据都符合规范。包括检查标签是否是有效的 Prometheus 标签，以及确保时间戳不会太旧或太新，或日志行不会太长。

### 预处理

目前， distributor改变传入数据的唯一方法是规范化标签。这意味着`{foo="bar", bazz="buzz"}`等同于`{bazz="buzz", foo="bar"}`，或者换句话说，对标签进行排序。这使得Loki可以正确地缓存和哈希它们。

### 限速

Distributor还可以基于每个租户的最大比特率对传入日志进行速率限制。它通过检查每个租户的限制并将其除以当前的distributor的数量来实现。这允许在集群级别为每个租户指定速率限制，并使我们能够向上或向下扩展distributor并相应地调整每个分发器的限制。例如，假设我们有 10 个distributor，租户 A 的速率限制为 10MB。在限制之前，每个分发器最多允许 1MB/秒。现在，假设另一个大租户加入集群，我们需要再启动 10 个distributor。现在 20 个distributor将将调整租户A的速率限制为`(10MB / 20 distributors) = 500KB/s`！这就是如何全局限制让允许 Loki 集群更简单、更安全的操作。

**注意：Distributor使用hood下的`ring`组件在其对等体中注册自己并获得活动distributors的总数。这是一个与环中ingesters使用的不同的“密钥”，来自distributor自己的环配置。**

### 转发

一旦distributor执行了所有的验证任务，它就会将数据转发到最终负责确认写入操作的ingester组件。

#### 复制因子

为了减少丢失任何单个ingester上的数据的可能性，distributor会向其中的一个*replication_factor*转发写入操作。复制允许在没有写入失败的情况下重新启动，并在某些情况下增加额外的数据丢失保护。松散地，对于推送到distributor的每个标签集（称为*流*），它将对标签进行哈希处理，并使用哈希值在环中查找复制因子ingester（这是一个公开的[分布式散列表](https://en.wikipedia.org/wiki/Distributed_hash_table)子组件）。然后它会尝试将相同的数据写入这些文件。如果写入成功少于仲裁次数，则此操作将出错。仲裁被定义为`floor(replication_factor / 2) + 1`。因此我们要求两次写入成功。如果写入成功的次数少于两次，则distributor会返回错误并可以重试写入。

**警告：还有一种极端情况，如果三个ingesters中有2个写入，我们会认为写入成功，这意味着在遭受数据丢失之前，我们只能容忍丢失一个ingester。**

不过，复制因子并不是防止数据丢失的唯一因素，可以说，如今它的主要目的是允许在部署和重新启动期间不间断地继续写入。`ingester`组件现在包含一个[预写日志](https://en.wikipedia.org/wiki/Write-ahead_logging)，该日志将传入的写入持久化到磁盘，以确保只要磁盘未损坏，它们就不会丢失。复制因子和 WAL 的互补性质确保数据不会丢失，除非两种机制都出现重大故障（即多个ingesters死亡并丢失或者损坏其磁盘）。

## 为什么它值得拥有自己的组件？

值得注意的是，distributor是一个无状态组件。这使得从ingester中不断扩展和卸载变得容易，ingester是写入流程上最关键的组件。独立扩展这些验证操作的能力意味着 Loki 还可以保护自己免受DoS攻击（无论是否是恶意），否则可能会使ingester过载。他们就像前门的保镖一样，确保每个人都穿着得体并收到邀请。它还允许我们根据前面描述的复制因子输出写入。